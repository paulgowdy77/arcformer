C:\Users/paul/Documents/arcformer\training\model.py:64: UserWarning: 1Torch was not compiled with flash attention. (Triggered internally at ..\aten\src\ATen\native\transformers\cuda\sdp_utils.cpp:263.)
  y = torch.nn.functional.scaled_dot_product_attention(q, k, v, attn_mask=None, dropout_p=self.dropout if self.training else 0, is_causal=True)
step 0: train loss 5.5583
iter 0: loss 5.5661, time 84416.80ms, mfu -100.00%
iter 1: loss 5.5657, time 67874.06ms, mfu -100.00%
iter 2: loss 5.5513, time 67560.28ms, mfu -100.00%
iter 3: loss 5.5176, time 68436.84ms, mfu -100.00%
step 4: train loss 5.5462
iter 4: loss 5.5497, time 89902.92ms, mfu -100.00%
iter 5: loss 5.5441, time 69097.68ms, mfu 0.07%
iter 6: loss 5.5318, time 69048.91ms, mfu 0.07%
iter 7: loss 5.5552, time 68304.81ms, mfu 0.07%
step 8: train loss 5.5235
iter 8: loss 5.5560, time 85615.98ms, mfu 0.07%
iter 9: loss 5.5109, time 68470.42ms, mfu 0.07%
iter 10: loss 5.5377, time 68610.13ms, mfu 0.07%
iter 11: loss 5.4801, time 68717.57ms, mfu 0.07%
step 12: train loss 5.4861
iter 12: loss 5.4917, time 87089.92ms, mfu 0.07%
iter 13: loss 5.4462, time 69877.33ms, mfu 0.07%
iter 14: loss 5.4623, time 68375.59ms, mfu 0.07%
iter 15: loss 5.4558, time 68347.44ms, mfu 0.07%
step 16: train loss 5.4191
iter 16: loss 5.3560, time 82600.97ms, mfu 0.06%
iter 17: loss 5.4126, time 66579.92ms, mfu 0.07%
iter 18: loss 5.3849, time 66650.22ms, mfu 0.07%
iter 19: loss 5.3849, time 66586.58ms, mfu 0.07%
step 20: train loss 5.3623
iter 20: loss 5.3798, time 83250.28ms, mfu 0.07%
iter 21: loss 5.3317, time 67183.64ms, mfu 0.07%
iter 22: loss 5.3718, time 66339.90ms, mfu 0.07%
iter 23: loss 5.3456, time 66189.05ms, mfu 0.07%
step 24: train loss 5.3226
iter 24: loss 5.3425, time 81978.24ms, mfu 0.07%
iter 25: loss 5.3111, time 65677.30ms, mfu 0.07%
iter 26: loss 5.2698, time 65782.25ms, mfu 0.07%
iter 27: loss 5.2461, time 68398.58ms, mfu 0.07%
step 28: train loss 5.2803
iter 28: loss 5.2768, time 84184.09ms, mfu 0.07%
iter 29: loss 5.2520, time 67464.02ms, mfu 0.07%
iter 30: loss 5.2449, time 67492.56ms, mfu 0.07%
iter 31: loss 5.2559, time 67118.81ms, mfu 0.07%
step 32: train loss 5.2499
iter 32: loss 5.2691, time 83433.14ms, mfu 0.07%
iter 33: loss 5.2056, time 67363.50ms, mfu 0.07%
iter 34: loss 5.2551, time 67248.59ms, mfu 0.07%
iter 35: loss 5.2334, time 67558.05ms, mfu 0.07%
step 36: train loss 5.2303
iter 36: loss 5.2016, time 84042.52ms, mfu 0.07%
iter 37: loss 5.2406, time 67962.34ms, mfu 0.07%
iter 38: loss 5.2217, time 68460.83ms, mfu 0.07%
iter 39: loss 5.2667, time 69086.88ms, mfu 0.07%
step 40: train loss 5.2037
iter 40: loss 5.2607, time 84366.77ms, mfu 0.06%
iter 41: loss 5.2069, time 67137.47ms, mfu 0.07%
iter 42: loss 5.1837, time 67174.42ms, mfu 0.07%
iter 43: loss 5.2018, time 67094.12ms, mfu 0.07%
step 44: train loss 5.1836
iter 44: loss 5.1651, time 83798.78ms, mfu 0.06%
iter 45: loss 5.1742, time 67701.47ms, mfu 0.07%
iter 46: loss 5.2106, time 67089.89ms, mfu 0.07%
iter 47: loss 5.1931, time 67034.55ms, mfu 0.07%
step 48: train loss 5.1511
iter 48: loss 5.1850, time 83467.79ms, mfu 0.07%
iter 49: loss 5.1426, time 66695.70ms, mfu 0.07%
iter 50: loss 5.1496, time 67118.09ms, mfu 0.07%
iter 51: loss 5.1419, time 68069.31ms, mfu 0.07%
step 52: train loss 5.1313
iter 52: loss 5.1415, time 84152.76ms, mfu 0.07%
iter 53: loss 5.1221, time 67761.06ms, mfu 0.07%
iter 54: loss 5.1338, time 68653.39ms, mfu 0.07%
iter 55: loss 5.0928, time 67703.22ms, mfu 0.07%
step 56: train loss 5.1031
iter 56: loss 5.0746, time 84285.58ms, mfu 0.06%
iter 57: loss 5.0830, time 67396.30ms, mfu 0.07%
iter 58: loss 5.1235, time 66877.70ms, mfu 0.07%
iter 59: loss 5.0826, time 67038.05ms, mfu 0.07%
step 60: train loss 5.0770
iter 60: loss 5.0390, time 83625.68ms, mfu 0.06%
iter 61: loss 5.0533, time 67051.84ms, mfu 0.07%
iter 62: loss 5.0946, time 67072.47ms, mfu 0.07%
iter 63: loss 4.9911, time 66941.11ms, mfu 0.07%
step 64: train loss 5.0410
iter 64: loss 5.0798, time 83450.45ms, mfu 0.07%
iter 65: loss 5.0620, time 67083.21ms, mfu 0.07%
iter 66: loss 5.0929, time 67166.73ms, mfu 0.07%
iter 67: loss 5.0356, time 66751.67ms, mfu 0.07%
step 68: train loss 5.0146
iter 68: loss 5.0070, time 83613.33ms, mfu 0.07%
iter 69: loss 5.0096, time 66809.87ms, mfu 0.07%
iter 70: loss 4.9851, time 67522.70ms, mfu 0.07%
iter 71: loss 5.0539, time 67699.74ms, mfu 0.07%
step 72: train loss 4.9519
iter 72: loss 4.9744, time 84156.22ms, mfu 0.07%
iter 73: loss 4.9473, time 66603.00ms, mfu 0.07%
iter 74: loss 4.9445, time 66890.20ms, mfu 0.07%
iter 75: loss 4.8928, time 67123.37ms, mfu 0.07%
step 76: train loss 4.9224
iter 76: loss 4.9543, time 83153.17ms, mfu 0.07%
iter 77: loss 4.9202, time 66854.17ms, mfu 0.07%
iter 78: loss 4.9586, time 67239.59ms, mfu 0.07%
iter 79: loss 4.8955, time 67488.03ms, mfu 0.07%
step 80: train loss 4.9237
iter 80: loss 4.9378, time 83468.29ms, mfu 0.07%
iter 81: loss 4.8222, time 67213.67ms, mfu 0.07%
iter 82: loss 4.8699, time 67161.35ms, mfu 0.07%
iter 83: loss 4.9146, time 67138.41ms, mfu 0.07%
step 84: train loss 4.8818
iter 84: loss 4.9267, time 83527.39ms, mfu 0.07%
iter 85: loss 4.8584, time 67568.82ms, mfu 0.07%
iter 86: loss 4.8985, time 67393.47ms, mfu 0.07%
iter 87: loss 4.8432, time 67576.12ms, mfu 0.07%
step 88: train loss 4.8576
iter 88: loss 4.8991, time 84748.03ms, mfu 0.07%
iter 89: loss 4.8527, time 67561.33ms, mfu 0.07%
iter 90: loss 4.8477, time 67236.87ms, mfu 0.07%
iter 91: loss 4.8029, time 67056.08ms, mfu 0.07%
step 92: train loss 4.8323
iter 92: loss 4.8013, time 83864.56ms, mfu 0.07%
iter 93: loss 4.8037, time 67392.10ms, mfu 0.07%
iter 94: loss 4.8474, time 67102.34ms, mfu 0.07%
iter 95: loss 4.8177, time 67629.63ms, mfu 0.07%
step 96: train loss 4.8183
iter 96: loss 4.8135, time 84524.67ms, mfu 0.07%
iter 97: loss 4.8309, time 68308.98ms, mfu 0.07%
iter 98: loss 4.7695, time 68165.82ms, mfu 0.07%
iter 99: loss 4.8263, time 67323.33ms, mfu 0.07%
step 100: train loss 4.7857
iter 100: loss 4.7394, time 83642.26ms, mfu 0.06%
iter 101: loss 4.7542, time 67251.33ms, mfu 0.07%
iter 102: loss 4.7143, time 67540.76ms, mfu 0.07%
iter 103: loss 4.7842, time 66981.12ms, mfu 0.07%
step 104: train loss 4.7630
iter 104: loss 4.7438, time 86694.94ms, mfu 0.06%
iter 105: loss 4.7813, time 68203.91ms, mfu 0.07%
iter 106: loss 4.7713, time 67044.29ms, mfu 0.07%
iter 107: loss 4.7233, time 66830.31ms, mfu 0.07%
step 108: train loss 4.7343
iter 108: loss 4.6177, time 82914.86ms, mfu 0.06%
iter 109: loss 4.6586, time 67101.45ms, mfu 0.07%
iter 110: loss 4.8131, time 67240.04ms, mfu 0.07%
iter 111: loss 4.7118, time 67109.90ms, mfu 0.07%
step 112: train loss 4.7016
iter 112: loss 4.8138, time 84925.71ms, mfu 0.06%
iter 113: loss 4.6338, time 68589.55ms, mfu 0.07%
iter 114: loss 4.6713, time 67214.66ms, mfu 0.07%
iter 115: loss 4.7421, time 66991.06ms, mfu 0.07%
step 116: train loss 4.6563
iter 116: loss 4.6932, time 83711.47ms, mfu 0.06%
iter 117: loss 4.6047, time 67222.94ms, mfu 0.07%
iter 118: loss 4.6951, time 66984.35ms, mfu 0.07%
iter 119: loss 4.6475, time 66685.41ms, mfu 0.07%
step 120: train loss 4.6451
iter 120: loss 4.7162, time 84364.05ms, mfu 0.07%
iter 121: loss 4.6862, time 68701.35ms, mfu 0.07%
iter 122: loss 4.6932, time 67917.30ms, mfu 0.07%
iter 123: loss 4.6622, time 68785.18ms, mfu 0.07%
step 124: train loss 4.6405
iter 124: loss 4.6435, time 83642.04ms, mfu 0.06%
iter 125: loss 4.6273, time 66981.30ms, mfu 0.07%
iter 126: loss 4.4925, time 66800.57ms, mfu 0.07%
iter 127: loss 4.5833, time 67086.98ms, mfu 0.07%
step 128: train loss 4.6039
iter 128: loss 4.5920, time 83704.24ms, mfu 0.06%
iter 129: loss 4.6085, time 67678.90ms, mfu 0.07%
iter 130: loss 4.5315, time 67109.24ms, mfu 0.07%
iter 131: loss 4.6388, time 69156.19ms, mfu 0.07%
step 132: train loss 4.5327
iter 132: loss 4.5823, time 84625.18ms, mfu 0.06%
iter 133: loss 4.6044, time 67829.25ms, mfu 0.07%
iter 134: loss 4.6152, time 67870.89ms, mfu 0.07%
iter 135: loss 4.6068, time 67869.36ms, mfu 0.07%
step 136: train loss 4.5590
iter 136: loss 4.4541, time 84266.36ms, mfu 0.06%
iter 137: loss 4.5243, time 68781.42ms, mfu 0.07%
iter 138: loss 4.4498, time 70459.08ms, mfu 0.07%
iter 139: loss 4.5362, time 68413.48ms, mfu 0.07%
step 140: train loss 4.5020
iter 140: loss 4.5138, time 84892.00ms, mfu 0.06%
iter 141: loss 4.5119, time 68439.85ms, mfu 0.06%
iter 142: loss 4.6489, time 68000.52ms, mfu 0.07%
iter 143: loss 4.4942, time 67910.20ms, mfu 0.07%
step 144: train loss 4.4991
iter 144: loss 4.4768, time 84218.20ms, mfu 0.06%
iter 145: loss 4.4558, time 68302.87ms, mfu 0.06%
iter 146: loss 4.3217, time 68143.88ms, mfu 0.07%
iter 147: loss 4.3501, time 67704.38ms, mfu 0.07%
step 148: train loss 4.4892
iter 148: loss 4.4993, time 84108.16ms, mfu 0.06%
iter 149: loss 4.4917, time 67458.54ms, mfu 0.06%
iter 150: loss 4.5126, time 67449.62ms, mfu 0.07%
iter 151: loss 4.2762, time 67635.86ms, mfu 0.07%
step 152: train loss 4.4561
iter 152: loss 4.4535, time 84339.33ms, mfu 0.06%
iter 153: loss 4.5467, time 67543.27ms, mfu 0.06%
iter 154: loss 4.4154, time 68237.05ms, mfu 0.07%
iter 155: loss 4.4394, time 68826.70ms, mfu 0.07%
step 156: train loss 4.4344
iter 156: loss 4.4507, time 84771.88ms, mfu 0.06%
iter 157: loss 4.4336, time 65989.92ms, mfu 0.07%
iter 158: loss 4.3623, time 65981.11ms, mfu 0.07%
iter 159: loss 4.4376, time 65967.87ms, mfu 0.07%
step 160: train loss 4.4030
iter 160: loss 4.3458, time 82576.86ms, mfu 0.07%
iter 161: loss 4.4594, time 66011.64ms, mfu 0.07%
iter 162: loss 4.3820, time 66417.77ms, mfu 0.07%
iter 163: loss 4.3956, time 65779.74ms, mfu 0.07%
step 164: train loss 4.3719
iter 164: loss 4.2970, time 82126.32ms, mfu 0.07%
iter 165: loss 4.3815, time 65927.23ms, mfu 0.07%
iter 166: loss 4.3034, time 66186.48ms, mfu 0.07%
iter 167: loss 4.2910, time 66135.29ms, mfu 0.07%
step 168: train loss 4.3940
iter 168: loss 4.1228, time 81865.05ms, mfu 0.07%
iter 169: loss 4.4482, time 65929.53ms, mfu 0.07%
iter 170: loss 4.2284, time 65848.96ms, mfu 0.07%
iter 171: loss 4.3399, time 67788.48ms, mfu 0.07%
step 172: train loss 4.3483
iter 172: loss 4.2545, time 83372.90ms, mfu 0.07%
iter 173: loss 4.3232, time 65903.04ms, mfu 0.07%
iter 174: loss 4.4410, time 65939.22ms, mfu 0.07%
iter 175: loss 4.2389, time 65769.32ms, mfu 0.07%
step 176: train loss 4.3507
iter 176: loss 4.4322, time 82104.21ms, mfu 0.07%
iter 177: loss 4.4805, time 65752.12ms, mfu 0.07%
iter 178: loss 4.3625, time 65690.29ms, mfu 0.07%
iter 179: loss 4.3314, time 66541.89ms, mfu 0.07%
step 180: train loss 4.2999
iter 180: loss 4.3791, time 82734.89ms, mfu 0.07%
iter 181: loss 4.3092, time 68341.81ms, mfu 0.07%
iter 182: loss 4.3590, time 68268.81ms, mfu 0.07%
Traceback (most recent call last):
  File "C:\Users\paul\Documents\arcformer\training\train.py", line 200, in <module>
    X, Y = get_batch()
           ^^^^^^^^^^^
  File "C:\Users\paul\Documents\arcformer\training\train.py", line 132, in get_batch
    x, y = next(iter(train_dataloader))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\paul\AppData\Local\Programs\Python\Python311\Lib\site-packages\torch\utils\data\dataloader.py", line 631, in __next__
    data = self._next_data()
           ^^^^^^^^^^^^^^^^^
  File "C:\Users\paul\AppData\Local\Programs\Python\Python311\Lib\site-packages\torch\utils\data\dataloader.py", line 674, in _next_data
    index = self._next_index()  # may raise StopIteration
            ^^^^^^^^^^^^^^^^^^
  File "C:\Users\paul\AppData\Local\Programs\Python\Python311\Lib\site-packages\torch\utils\data\dataloader.py", line 621, in _next_index
    return next(self._sampler_iter)  # may raise StopIteration
           ^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\paul\AppData\Local\Programs\Python\Python311\Lib\site-packages\torch\utils\data\sampler.py", line 287, in __iter__
    for idx in self.sampler:
  File "C:\Users\paul\AppData\Local\Programs\Python\Python311\Lib\site-packages\torch\utils\data\sampler.py", line 167, in __iter__
    yield from torch.randperm(n, generator=generator).tolist()
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
KeyboardInterrupt